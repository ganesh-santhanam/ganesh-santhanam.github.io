---
title: "Bayesian"
date: 2018-01-28
mathjax: "true"
---

Python code block:
```python
import theano.tensor as tt  # pymc devs are discussing new backends
import pymc3 as pm

n_hidden = 20

with pm.Model() as nn_model:
  # Input -> Layer 1
  weights_1 = pm.Normal('w_1', mu=0, sd=1,
                        shape=(ann_input.shape[1], n_hidden),
                        testval=init_1)
  acts_1 = pm.Deterministic('activations_1',
                            tt.tanh(tt.dot(ann_input, weights_1)))

  # Layer 1 -> Layer 2
  weights_2 = pm.Normal('w_2', mu=0, sd=1,
                        shape=(n_hidden, n_hidden),
                        testval=init_2)
  acts_2 = pm.Deterministic('activations_2',
                            tt.tanh(tt.dot(acts_1, weights_2)))

  # Layer 2 -> Output Layer
  weights_out = pm.Normal('w_out', mu=0, sd=1,
                          shape=(n_hidden, ann_output.shape[1]),
                          testval=init_out)
  acts_out = pm.Deterministic('activations_out',
                              tt.nnet.softmax(tt.dot(acts_2, weights_out)))  # noqa

  # Define likelihood
  out = pm.Multinomial('likelihood', n=1, p=acts_out,
                       observed=ann_output)

with nn_model:
  s = theano.shared(pm.floatX(1.1))
  inference = pm.ADVI(cost_part_grad_scale=s)  # approximate inference done using ADVI
  approx = pm.fit(100000, method=inference)
  trace = approx.sample(5000)
```

$$
\tanh x = \frac { e ^ { 2 x } - 1 } { e ^ { 2 x } + 1 }
$$
Here's some math:

$$z=x+y$$

Latex Math

tanh
$$ Z = \tanh \left( f \left( X , w _ { 1 } , b _ { 1 } \right) \right) $$

Normal Prior
$$ \begin{array} { r l } { w _ { 1 , i } } & { \sim N ( 0,1 ) } \\ { b _ { 1 , i } } & { \sim N ( 0,1 ) } \end{array}
$$


The data used in this report was taken from the UC Irvine Machine Learning Repository .

This "Covertype" Dataset
Input: 66 cartographic variables
Output: one of 7 forest cover types
  The data setconsisted of 15,120 samples of 30m x 30m patches of forest located in northern Colorado’s Roosevelt NationalForest.   Certain  attributes  lend  themselves  well  to  human  interpretation.   For  example,  the  correlationbetween aspect (compass direction of slope face) and sunlight intensity makes intuitive sense, seeing as sometrees may thrive with heavy morning sun.  Each data sample includes 54 attributes:  elevation (in meters),slope, aspect (compass direction of slope face), vertical distance from water, horizontal distance from water,sunlight intensity at 9am,  12pm,  and 3pm,  4 binary wilderness area designators,  and 40 binary soil typedesignators.  In some tests, the 44 binary attributes were ommitted.  Each sample was classified into one ofseven forest cover types: Spruce/Fir, Lodgepole Pine, Ponderosa Pine, Cottonwood/Willow, Aspen, DouglasFir, or Krummholz.  There are 2,160 samples classified as each of the seven cover types


<p align="center">
<img src="https://imgur.com/IW3YlC1.jpg">
</p>

<center>
*Neural Network with 2 hidden layers*
</center>

This is a shallow 2 hidden layer neural network with 20 hidden nodes in layer 2 and 3. It has 66 inputs and outputs one of 7 classes.

As it is a shallow network Relu is not used. Instead Tanh is used as it has stronger gradients as its range is from [-1,1]

<p align="center">
<img src="https://imgur.com/uQLulwa.jpg">
</p>

<center>
*Posterior estimation*
</center>


Probabilistic Programming in Python. Provides:

statistical distributions
sampling algorithms

and uses theano for auto Differentiation

<p align="center">
<img src="https://imgur.com/rlj30rc.jpg">
</p>

<center>
*PYMC3*
</center>

Automatic Differentiation Variational Inference
we maximizethe evidence lower bound (elbo) as we cannot directly maximize the KL divergence as it lacks an analytic form as it involves the posterior.



$$
\mathcal { L } ( \phi ) = \mathbb { E } _ { q ( \theta ) } [ \log p ( \mathbf { x } , \theta ) ] - \mathbb { E } _ { q ( \theta ) } [ \log q ( \theta ; \phi ) ]
$$


The first term is an expectation of the joint density under the approximation, and the second is theentropy of the variational density. Theelbois equal to the negativekldivergence up to the constantlogp(x). Maximizing theelbominimizes thekldivergence (Jordan et al., 1999; Bishop, 2006)


We would like to optimize using stochastic gradient ascent to maximize the elbo and use automatic differentiation to compute gradients.

$$
\phi ^ { * } = \underset { \phi \in \Phi } { \arg \max } \mathcal { L } ( \phi )
$$

However we cannot directly use automatic differentiation on the elbo. This is because the elbo involves an intractable expectation.

Hence we begin by transforming the support of the latent variables θ such that they live in the real coordinate space

<p align="center">
<img src="https://imgur.com/O8cQbDP.jpg">
</p>

<center>
*Transforming Latent variable to real coordinate space*
</center>

 Then we employ one final transformation: elliptical standardization.

$$
q ( \boldsymbol { \eta } ) = \text { Normal } ( \boldsymbol { \eta } | \mathbf { 0 } , \mathbf { I } ) = \prod _ { k = 1 } ^ { K } \mathrm { Normal } \left( \eta _ { k } | 0,1 \right)
$$

The standardization transforms the variational problem from Equation into

$$
\phi ^ { * } = \underset { \phi } { \arg \max } \mathbb { E } _ { \mathrm { N } ( \eta ; 0,1 ) } \left[ \log p \left( \mathbf { x } , T ^ { - 1 } \left( S _ { \phi } ^ { - 1 } ( \boldsymbol { \eta } ) \right) \right) + \log \left| \operatorname { det } J _ { T ^ { - 1 } } \left( S _ { \phi } ^ { - 1 } ( \eta ) \right) \right| \right] + \mathbb { H } [ q ( \zeta ; \phi ) ]
$$

<p align="center">
<img src="https://imgur.com/BHsOwsg.jpg">
</p>

<center>
*Real coordinate to standardized space*
</center>

We now reach the final step: stochastic optimization of the variational objective function. Since the expectation is no longer dependent on φ, we can directly calculate its gradient

$$
\nabla _ { \mu } \mathcal { L } = \mathbb { E } _ { \mathrm { N } ( \eta ) } \left[ \nabla _ { \boldsymbol { \theta } } \log p ( \mathbf { x } , \boldsymbol { \theta } ) \nabla _ { \zeta } T ^ { - 1 } ( \boldsymbol { \zeta } ) + \nabla _ { \zeta } \log \left| \operatorname { det } J _ { T ^ { - 1 } } ( \boldsymbol { \zeta } ) \right| \right]
$$
We obtain gradients with respect to ω (mean-field) and L (full-rank) in a similar fashion

$$
\nabla _ { \boldsymbol { \omega } } \mathcal { L } = \mathbb { E } _ { \mathrm { N } ( \boldsymbol { \eta } ) } \left[ \left( \nabla _ { \boldsymbol { \theta } } \log p ( \mathbf { x } , \boldsymbol { \theta } ) \nabla _ { \zeta } T ^ { - 1 } ( \boldsymbol { \zeta } ) + \nabla _ { \zeta } \log \left| \operatorname { det } J _ { T ^ { - 1 } } ( \boldsymbol { \zeta } ) \right| \right) \boldsymbol { \eta } ^ { \top } \operatorname { diag } ( \exp ( \boldsymbol { \omega } ) ) \right] + \mathbf { 1 }
$$

$$
\nabla _ { \mathbf { L } } \mathcal { L } = \mathbb { E } _ { \mathrm { N } ( \eta ) } \left[ \left( \nabla _ { \boldsymbol { \theta } } \log p ( \mathbf { x } , \boldsymbol { \theta } ) \nabla _ { \zeta } T ^ { - 1 } ( \boldsymbol { \zeta } ) + \nabla _ { \zeta } \log \left| \operatorname { det } J _ { T ^ { - 1 } } ( \zeta ) \right| \right) \eta ^ { \top } \right] + \left( \mathbf { L } ^ { - 1 } \right) ^ { \top }
$$


We visualize the weights of the outputs.

<p align="center">
<img src="https://imgur.com/LpYJ6jI.jpg">
</p>
<center>
Output layer weights
</center>

We can compute both the point estimates as well as the bayesian estimates.

<p align="center">
<img src="https://imgur.com/t0L4iZS.jpg">
</p>

<center>
*Point Estimates*
</center>


<p align="center">
<img src="https://imgur.com/IGSOEdn.jpg">
</p>

<center>
Bayesian Estimates with uncertainity
</center>

Though the point estimates indicate clear classification between the class boundaries the bayesian estimates tell a very different story. It appears that the model has difficulty in differentiating between class 1 and 2. Hence we should try to get more data for these 2 classes to predict with more confidence.

# References

1. Blackard, Jock A., Dean, Denis J. “Comparative accuracies of artifical neural networks and discriminantanalysis  in  predicting  foorest  cover  types  from  cartogrpahic  variables”.  Computers  and  Electronics  inAgriculture (1999).

2. Alp  Kucukelbir,  Dustin  Tran,  Rajesh  Ranganath,  Andrew  Gelman,  and  David  M.  Blei.Automatic differentiation variational inference.Journal  of  Machine  Learning  Research,18(14):1–45, 2017

3. Frederic Bastien,  Pascal Lamblin, Razvan Pascanu, James Bergstra, Ian Goodfellow, Ar-naud  Bergeron,  Nicolas  Bouchard,  David  Warde-Farley,  and  Yoshua  Bengio.   Theano:new features and speed improvements.  Deep Learning and Unsupervised Feature Learn-ing NIPS 2012 Workshop, 2012

4. John Salvatier, Thomas V Wiecki, and Christopher Fonnesbeck. Probabilistic programmingin Python using PyMC3.PeerJ Computer Science, 2:e55, 2016
